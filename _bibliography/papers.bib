%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Marco Rospocher at 2024-05-28 09:16:03 +0200 


%% Saved with string encoding Unicode (UTF-8) 


@book{montoya_2023,
  title={Books without barriers: a practical guide to inclusive publishing},
  author={Ganner, J. and Mrva-Montoya, A. and Park, M. and Duncan, K.},
  html={https://www.iped-editors.org/resources-for-editors/books-without-barriers/},
  year={2023},
  publisher={Institute of Professional Editors and Australian Publishers Association}
}

@book{bellacosa2024libri,
  title={Libri senza barriere. Percorsi di editoria accessibile e inclusiva},
  author={Bellacosa, L. and Mussinelli, C. and Wolf, M.},
  isbn={9788896120521},
  series={Libri di libri},
  html={https://books.google.it/books?id=ndCu0AEACAAJ},
  year={2024},
  publisher={Edizioni Santa Caterina}
}


@article{moretti_conjectures_2000,
	title = {Conjectures on world literature},
	volume = {1},
	issn = {0028-6060},
	journal = {New left review},
	author = {Moretti, Franco},
	year = {2000},
	pages = {54--68},
}

@book{moretti_distant_2013,
	address = {London},
	title = {Distant reading},
	isbn = {978-1-78168-084-1},
	language = {en},
	publisher = {Verso},
	author = {Moretti, Franco},
	year = {2013}
}

@article{rebora_digital_2021,
	title = {Digital humanities and digital social reading},
	volume = {36},
	copyright = {All rights reserved},
	issn = {2055-7671, 2055-768X},
	url = {https://academic.oup.com/dsh/article/36/Supplement_2/ii230/6421802},
	doi = {10.1093/llc/fqab020},
	language = {en},
	number = {Supplement\_2},
	urldate = {2022-01-28},
	journal = {Digital Scholarship in the Humanities},
	author = {Rebora, Simone and Boot, Peter and Pianzola, Federico and Gasser, Brigitte and Herrmann, J Berenike and Kraxenberger, Maria and Kuijpers, Moniek M and Lauer, Gerhard and Lendvai, Piroska and Messerli, Thomas C and Sorrentino, Pasqualina},
	month = nov,
	year = {2021},
	pages = {ii230--ii250},
}

@article{salgaro_literary_2022,
	title = {Literary value in the era of big data. {Operationalizing} critical distance in professional and non-professional reviews},
	volume = {7},
	issn = {2371-4549},
	url = {https://culturalanalytics.org/article/36446-literary-value-in-the-era-of-big-data-operationalizing-critical-distance-in-professional-and-non-professional-reviews},
	doi = {10.22148/001c.36446},
	language = {en},
	number = {2},
	urldate = {2022-10-01},
	journal = {Journal of Cultural Analytics},
	author = {Salgaro, Massimo},
	month = jun,
	year = {2022},
}




@article{2024capp,
	abstract = {This paper is based on an ongoing project on Diversity, Inclusivity, Accessibility in Digital Scholarly Editing (DIA-DSE and in our view it should be seen as a place of critical assessment of existing digital scholarly editions (DSE) and discussion for future developments and improvement. Our idea is the result of a bigger initiative based at the University of Verona (Italy) on the topic Inclusive Humanities: Perspectives of Development in Research and Teaching Foreign Languages and Literatures [1]. In its essence it tries to respond to some of the goals envisaged by globally relevant agendas and strategic plans which put in the foreground the challenges posed by our time and focuses on the idea that a knowledge society like ours needs to develop an open model of science. This novel model pleads for an accessible science and through innovative methodologies seeks to involve wide, inclusive and diverse agents, contents, and targets into the scientific discourse. In this context, our aim is to investigate from the perspective of Diversity, Inclusivity and Accessibility (which we call by the acronym DIA) a traditional field of study, that is philology and textual criticism, in its very ultimate development: Digital Scholarly Editions (DSE). The field of DSEs raises nowadays the following questions: Do DSE projects consider Diversity, Inclusivity and Accessibility? If so, how much and how do they do this? To try to provide an answer, in the context of DIA-DSE project we will build a corpus of existing resources and we will try to assess their DIA degree according to different parameters. In the long term, after data collection and analysis, a ranking of diverse, inclusive and accessible resources will be defined. These results will be followed by a survey that will be disseminated among the scholarly community and users, with the objective engage in an open critical discussion, to raise awareness and to gain suggestions for the creation of DIA-DSE guidelines that will be published and promoted at the end of the project.},
	author = {Anna Cappellotto and Raffaele Cioffi},
	copyright = {Copyright (c) 2024 Anna Cappellotto, Raffaele Cioffi},
	date-added = {2024-05-28 09:15:41 +0200},
	date-modified = {2024-05-28 09:15:58 +0200},
	doi = {10.55492/dhasa.v5i1.5012},
	file = {Full Text PDF:/Users/annacappellotto/Zotero/storage/NN52Z3EJ/Cappellotto and Cioffi - 2024 - Towards an Inclusive and Accessible Digital Schola.pdf:application/pdf},
	issn = {3006-6492},
	journal = {Journal of the Digital Humanities Association of Southern Africa},
	keywords = {Digital Scholarly Editions, Inclusivity, Web Accessibility Initiative},
	language = {en},
	month = feb,
	number = {1},
	shorttitle = {Towards an {Inclusive} and {Accessible} {Digital} {Scholarly} {Editing}},
	title = {Towards an {Inclusive} and {Accessible} {Digital} {Scholarly} {Editing}: {A} {Critical} {Assessment}},
	html = {https://upjournals.up.ac.za/index.php/dhasa/article/view/5012},
	urldate = {2024-05-27},
	volume = {5},
	year = {2024},
	bdsk-url-1 = {https://upjournals.up.ac.za/index.php/dhasa/article/view/5012},
	bdsk-url-2 = {https://doi.org/10.55492/dhasa.v5i1.5012}}

@inproceedings{2023moot,
	author = {Manuel Boschiero and Marco Rospocher and Olga Lucia Forlani and Silvano Pasquali},
	booktitle = {Atti del MoodleMoot Italia 2023},
	date-added = {2024-02-29 17:01:17 +0100},
	date-modified = {2024-02-29 17:02:17 +0100},
	html = {https://eventi.aium.it/event/1/contributions/44/attachments/12/17/Pagine%20da%20MoodleMoot%20Italia%202023%20-%20Atti%20del%20Convegno-9.pdf},
	isbn = {978-88-907493-9-1},
	pages = {61--70},
	publisher = {MediaTouch 2000 (In co-edizione con Associazione Italiana Utenti Moodle A.p.s (AIUM), Universit{\`a} degli Studi di Firenze)},
	title = {Migliorare l'accessibilit{\`a} dei materiali didattici digitali nel contesto universitario: un caso di studio},
	year = {2024},
	bdsk-url-1 = {https://eventi.aium.it/event/1/contributions/44/attachments/12/17/Pagine%20da%20MoodleMoot%20Italia%202023%20-%20Atti%20del%20Convegno-9.pdf}}

@article{2023cbm,
	abstract = {The automatic extraction of procedural surgical knowledge from surgery manuals, academic papers or other high-quality textual resources, is of the utmost importance to develop knowledge-based clinical decision support systems, to automatically execute some procedure's step or to summarize the procedural information, spread throughout the texts, in a structured form usable as a study resource by medical students. In this work, we propose a first benchmark on extracting detailed surgical actions from available intervention procedure textbooks and papers. We frame the problem as a Semantic Role Labeling task. Exploiting a manually annotated dataset, we apply different Transformer-based information extraction methods. Starting from RoBERTa and BioMedRoBERTa pre-trained language models, we first investigate a zero-shot scenario and compare the obtained results with a full fine-tuning setting. We then introduce a new ad-hoc surgical language model, named SurgicBERTa, pre-trained on a large collection of surgical materials, and we compare it with the previous ones. In the assessment, we explore different dataset splits (one in-domain and two out-of-domain) and we investigate also the effectiveness of the approach in a few-shot learning scenario. Performance are evaluated on three correlated sub-tasks: predicate disambiguation, semantic argument disambiguation and predicate-argument disambiguation. Results show that the fine-tuning of a pre-trained domain-specific language model achieves the highest performance on all splits and on all sub-tasks. All models are publicly released.},
	author = {Marco Bombieri and Marco Rospocher and Simone Paolo Ponzetto and Paolo Fiorini},
	date-added = {2022-12-14 12:57:37 +0100},
	date-modified = {2023-03-22 08:03:32 +0100},
	doi = {10.1016/j.compbiomed.2022.106415},
	html = {https://www.sciencedirect.com/science/article/pii/S0010482522011234},
	issn = {0010-4825},
	journal = {Computers in Biology and Medicine},
	keywords = {Semantic role labeling, Surgical data science, Procedural knowledge, Information extraction, Natural language processing},
	pages = {106415},
	scopus = {2-s2.0-85144338000},
	title = {Machine understanding surgical actions from intervention procedure textbooks},
	volume = {152},
	wos = {WOS:000900243500001},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0010482522011234},
	bdsk-url-2 = {https://doi.org/10.1016/j.compbiomed.2022.106415}}

@article{2021eswa,
	abstract = {In this paper, we investigate the problem of automatically detecting explicit song lyrics, i.e., determining if the lyrics of a given song could be offensive or unsuitable for children. The problem can be framed as a binary classification task, and in this work we propose to tackle it with the fastText classifier, an efficient linear classification model leveraging a peculiar distributional text representation that, by exploiting subword information in building the embeddings of the words, enables to cope with words not seen at training time. We assess the performance of the fastText classifier and word representations with a lyrics dataset of over 800K songs, annotated with explicit information, that we assembled from publicly available resources. The evaluation shows that the fastText classifier is effective for explicit lyrics detection, substantially outperforming a reference approach for the task, and that the subword information effectively contributes to this result.},
	author = {Marco Rospocher},
	date-added = {2020-08-19 16:43:36 +0200},
	date-modified = {2021-05-23 09:58:39 +0200},
	doi = {10.1016/j.eswa.2020.113749},
	html = {http://www.sciencedirect.com/science/article/pii/S095741742030573X},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Word embeddings, Text classification, Explicit content detection},
	scopus = {2-s2.0-85089102640},
	title = {Explicit song lyrics detection with subword-enriched word embeddings},
	volume = {163},
	wos = {WOS:000582115400006},
	year = {2021},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/S095741742030573X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113749}}

@article{2020jws,
	abstract = {In this work we address the problem of extracting quality entity knowledge from natural language text, an important task for the automatic construction of knowledge graphs from unstructured content. More in details, we investigate the benefit of performing a joint posterior revision, driven by ontological background knowledge, of the annotations resulting from natural language processing (NLP) entity analyses such as named entity recognition and classification (NERC) and entity linking (EL). The revision is performed via a probabilistic model, called jpark, that given the candidate annotations independently identified by NERC and EL tools on the same textual entity mention, reconsiders the best annotation choice performed by the tools in light of the coherence of the candidate annotations with the ontological knowledge. The model can be explicitly instructed to handle the information that an entity can potentially be NIL (i.e., lacking a corresponding referent in the target linking knowledge base), exploiting it for predicting the best NERC and EL annotation combination. We present a comprehensive evaluation of jpark along various dimensions, comparing its performances with and without exploiting NIL information, as well as the usage of three different background knowledge resources (YAGO, DBpedia, and Wikidata) to build the model. The evaluation, conducted using different tools (the popular Stanford NER and DBpedia Spotlight, as well as the more recent Flair NER and End-to-End Neural EL) with three reference datasets (AIDA, MEANTIME, and TAC-KBP), empirically confirms the capability of the model to improve the quality of the annotations of the given tools, and thus their performances on the tasks they are designed for.},
	author = {Marco Rospocher and Francesco Corcoglioniti},
	date-added = {2020-10-12 11:07:50 +0200},
	date-modified = {2021-01-08 20:00:59 +0100},
	doi = {10.1016/j.websem.2020.100617},
	html = {http://www.sciencedirect.com/science/article/pii/S1570826820300500},
	issn = {1570-8268},
	journal = {Journal of Web Semantics},
	pages = {100617},
	scopus = {2-s2.0-85093703848},
	title = {Knowledge-driven joint posterior revision of named entity classification and linking},
	volume = {65},
	wos = {WOS:000598338300006},
	year = {2020},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/S1570826820300500},
	bdsk-url-2 = {https://doi.org/10.1016/j.websem.2020.100617}}

@article{2019lre,
	author = {Marco Rospocher and Francesco Corcoglioniti and Alessio {Palmero Aprosio}},
	date-added = {2018-11-16 09:46:25 +0100},
	date-modified = {2019-09-20 08:32:29 +0200},
	doi = {10.1007/s10579-018-9437-8},
	html = {https://link.springer.com/article/10.1007/s10579-018-9437-8},
	journal = {Language Resources and Evaluation},
	manuscript = {https://marcorospocher.com/files/pubs/ta_lre.pdf},
	number = {3},
	pages = {499--524},
	scopus = {2-s2.0-85057966885},
	title = {PreMOn: LODifing Linguistic Predicate Models},
	volume = {53},
	wos = {WOS:000482894600007},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s10579-018-9437-8}}

@article{2016jws,
	author = {Marco Rospocher and Marieke van Erp and Piek Vossen and Antske Fokkens and Itziar Aldabe and German Rigau and Aitor Soroa and Thomas Ploeger and Tessel Bogaard},
	date-added = {2018-07-17 12:58:23 +0000},
	date-modified = {2019-03-01 08:26:22 +0100},
	doi = {10.1016/j.websem.2015.12.004},
	html = {http://www.sciencedirect.com/science/article/pii/S1570826815001456},
	issn = {1570-8268},
	journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
	manuscript = {https://marcorospocher.com/files/pubs/2016jws.pdf},
	pages = {132--151},
	publisher = {Elsevier B.V.},
	scopus = {2-s2.0-84994652650},
	title = {Building Event-Centric Knowledge Graphs from News},
	volume = {37--38},
	wos = {WOS:000376457600009},
	year = {2016},
	bdsk-url-1 = {http://dx.doi.org/10.1016/j.websem.2015.12.004}}

@article{2016tkde,
	abstract = {We present an approach for ontology population from natural language English texts that extracts RDF triples according to FrameBase, a Semantic Web ontology derived from FrameNet. Processing is decoupled in two independently-tunable phases. First, text is processed by several NLP tasks, including Semantic Role Labeling (SRL), whose results are integrated in an RDF graph of mentions, i.e., snippets of text denoting some entity/fact. Then, the mention graph is processed with SPARQL-like rules using a specifically created mapping resource from NomBank/PropBank/FrameNet annotations to FrameBase concepts, producing a knowledge graph whose content is linked to DBpedia and organized around semantic frames, i.e., prototypical descriptions of events and situations. A single RDF/OWL representation is used where each triple is related to the mentions/tools it comes from. We implemented the approach in PIKES, an open source tool that combines two complementary SRL systems and provides a working online demo. We evaluated PIKES on a manually annotated gold standard, assessing precision/recall in (i) populating FrameBase ontology, and (ii) extracting semantic frames modeled after standard predicate models, for comparison with state-of-the-art tools for the Semantic Web. We also evaluated (iii) sampled precision and execution times on a large corpus of 110 K Wikipedia-like pages.},
	author = {Francesco Corcoglioniti and Marco Rospocher and Alessio {Palmero Aprosio}},
	date-added = {2018-07-17 12:58:23 +0000},
	date-modified = {2019-03-01 08:26:54 +0100},
	doi = {10.1109/TKDE.2016.2602206},
	html = {https://ieeexplore.ieee.org/document/7549111/},
	issn = {1041-4347},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Labeling;Natural language processing;Ontologies;Pragmatics;Resource description framework;Semantics;Statistics;FrameBase;Ontology population;Semantic Web;natural language processing;semantic role labeling},
	manuscript = {https://marcorospocher.com/files/pubs/2016tkde.pdf},
	number = {12},
	pages = {3261--3275},
	publisher = {{IEEE}},
	scopus = {2-s2.0-84996478366},
	title = {Frame-Based Ontology Population with PIKES},
	volume = {28},
	wos = {WOS:000388214700010},
	year = {2016},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TKDE.2016.2602206}}

@article{2019swj,
	abstract = {We present an approach for ontology population from natural language English texts that extracts RDF triples according to FrameBase, a Semantic Web ontology derived from FrameNet. Processing is decoupled in two independently-tunable phases. First, text is processed by several NLP tasks, including Semantic Role Labeling (SRL), whose results are integrated in an RDF graph of mentions, i.e., snippets of text denoting some entity/fact. Then, the mention graph is processed with SPARQL-like rules using a specifically created mapping resource from NomBank/PropBank/FrameNet annotations to FrameBase concepts, producing a knowledge graph whose content is linked to DBpedia and organized around semantic frames, i.e., prototypical descriptions of events and situations. A single RDF/OWL representation is used where each triple is related to the mentions/tools it comes from. We implemented the approach in PIKES, an open source tool that combines two complementary SRL systems and provides a working online demo. We evaluated PIKES on a manually annotated gold standard, assessing precision/recall in (i) populating FrameBase ontology, and (ii) extracting semantic frames modeled after standard predicate models, for comparison with state-of-the-art tools for the Semantic Web. We also evaluated (iii) sampled precision and execution times on a large corpus of 110 K Wikipedia-like pages.},
	author = {Marco Rospocher and Francesco Corcoglioniti and Mauro Dragoni},
	date-added = {2018-08-23 14:20:40 +0200},
	date-modified = {2019-07-18 09:26:20 +0200},
	doi = {10.3233/SW-180325},
	html = {https://content.iospress.com/articles/semantic-web/sw325},
	issn = {2210-4968},
	journal = {Semantic Web - Interoperability, Usability, Applicability},
	manuscript = {https://marcorospocher.com/files/pubs/2019swj.pdf},
	number = {4},
	pages = {753--778},
	scopus = {2-s2.0-85066428245},
	title = {Boosting Document Retrieval with Knowledge Extraction and Linked Data},
	volume = {10},
	wos = {WOS:000488080900005},
	year = {2019},
	bdsk-url-1 = {http://semantic-web-journal.org/content/boosting-document-retrieval-knowledge-extraction-and-linked-data-1}}

@article{2021ijdl,
	abstract = {Metadata are fundamental for the indexing, browsing and retrieval of cultural heritage resources in repositories, digital libraries and catalogues. In order to be effectively exploited, metadata information has to meet some quality standards, typically defined in the collection usage guidelines. As manually checking the quality of metadata in a repository may not be affordable, especially in large collections, in this paper we specifically address the problem of automatically assessing the quality of metadata, focusing in particular on textual descriptions of cultural heritage items. We describe a novel approach based on machine learning that tackles this problem by framing it as a binary text classification task aimed at evaluating the accuracy of textual descriptions. We report our assessment of different classifiers using a new dataset that we developed, containing more than 100K descriptions. The dataset was extracted from different collections and domains from the Italian digital library ``Cultura Italia''and was annotated with accuracy information in terms of compliance with the cataloguing guidelines. The results empirically confirm that our proposed approach can effectively support curators (F1 ~0.85) in assessing the quality of the textual descriptions of the records in their collections and provide some insights into how training data, specifically their size and domain, can affect classification performance.},
	author = {Matteo Lorenzini and Marco Rospocher and Sara Tonelli},
	da = {2021/04/23},
	date-added = {2021-04-29 08:52:43 +0200},
	date-modified = {2021-09-17 17:47:06 +0200},
	doi = {10.1007/s00799-021-00302-1},
	id = {Lorenzini2021},
	isbn = {1432-1300},
	journal = {International Journal on Digital Libraries},
	number = {2},
	pages = {217--231},
	scopus = {2-s2.0-85105217980},
	title = {Automatically evaluating the quality of textual descriptions in cultural heritage records},
	ty = {JOUR},
	html = {https://doi.org/10.1007/s00799-021-00302-1},
	volume = {22},
	wos = {WOS:000642397400001},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s00799-021-00302-1}}

@article{2025ieeeAcc,
 author = {Marco Bombieri and Simone Paolo Ponzetto and Marco Rospocher},
 bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2025.3589112},
 date-added = {2025-07-17 09:30:04 +0200},
 date-modified = {2025-08-08 15:11:55 +0200},
 doi = {10.1109/ACCESS.2025.3589112},
 journal = {IEEE Access},
 keywords = {Large Language Models;Jailbreaking;Society;Ethics},
 pages = {126418-126431},
 scopus = {2-s2.0-105011159029},
 title = {The Dangerous Effects of a Frustratingly Easy LLMs Jailbreak Attack},
 volume = {13},
 wos = {WOS:001534557100004},
 html = {https://doi.org/10.1109/ACCESS.2025.3589112},
 year = {2025}
}

@inproceedings{2025clicitB,
 author = {Marco Bombieri and Simone Paolo Ponzetto and Marco Rospocher},
 booktitle = {CLiC-it 2025 Proceedings},
 date-added = {2025-08-30 15:44:38 +0200},
 date-modified = {2025-08-30 15:45:02 +0200},
 title = {Do LLMs Authentically Represent Affective Experiences of People with Disabilities on Social Media?},
 year = {2025},
 html = {https://clic2025.unica.it/wp-content/uploads/2025/09/8_main_long.pdf}
}


@article{2025inf,
	abstract = {In this paper, we introduce a survey-based methodology to audit LLM-generated personas by simulating 200 US residents and collecting responses to socio-demographic questions in a zero-shot setting. We investigate whether LLMs default to standardized profiles, how these profiles differ across models, and how conditioning on specific attributes affects the resulting portrayals. Our findings reveal that LLMs often produce homogenized personas that underrepresent demographic diversity and that conditioning on attributes such as gender, ethnicity, or disability may trigger stereotypical shifts. These results highlight implicit biases in LLMs and underscore the need for systematic approaches to evaluate and mitigate fairness risks in model outputs.},
	article-number = {931},
	author = {Marco Bombieri and Marco Rospocher},
	date-added = {2025-10-26 11:04:37 +0100},
	date-modified = {2025-10-26 11:05:37 +0100},
	doi = {10.3390/info16110931},
	issn = {2078-2489},
	journal = {Information},
	number = {11},
	title = {Mining Impersonification Bias in LLMs via Survey Filling},
	volume = {16},
	year = {2025},
	html = {https://doi.org/10.3390/info16110931},
	bdsk-url-1 = {https://www.mdpi.com/2078-2489/16/11/931},
	bdsk-url-2 = {https://doi.org/10.3390/info16110931}}

